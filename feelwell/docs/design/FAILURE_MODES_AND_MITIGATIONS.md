# Failure Modes and Mitigations Analysis
## Feelwell Mental Health AI Triage System - V2 Design

**Document Purpose:** Comprehensive analysis of what can go wrong and how we prevent or detect it.
**Last Updated:** 2026-01-17
**Classification:** Technical controls, monitoring mechanisms, and operational procedures

---

## How to Read This Document

Each failure mode is classified by mitigation approach:
- **ðŸ”’ Technical Control:** Automated system prevents the failure
- **ðŸ“Š Monitoring Mechanism:** System detects and alerts when failure occurs
- **ðŸ“‹ Process Control:** Human procedures and policies required

---

## 1. TRUST & TRANSPARENCY FAILURES

### 1.1 Black Box Decision Making
**What Goes Wrong:** Counselors receive risk scores but can't understand how the system reached conclusions. They either ignore the system entirely or blindly follow it.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Explainability Features:**
  - Show which specific conversation excerpts triggered each clinical marker (with student permission)
  - Display confidence scores for each assessment (e.g., "PHQ-9 Item 2: 85% confidence")
  - Provide reasoning trace: "Flagged due to sleep disturbance mentioned 3 times over 2 sessions"
- **Counselor Feedback Loop:**
  - "Was this flag helpful?" button on every alert
  - Track counselor override rate (if >30% override a specific marker type, recalibrate)
- **Audit Trail Transparency:**
  - Counselors can see full decision logic for any assessment they view
  - Version control for which model/ruleset was used

**Success Metric:** Counselor trust score >4/5, override rate <20%

---

### 1.2 Model Performance Degradation
**What Goes Wrong:** LLM provider updates their model, or student language evolves (new slang), and accuracy silently drops from 95% to 60% over months. Nobody notices.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Continuous Evaluation Pipeline:**
  - Run golden test set (MentalChat16K + custom cases) nightly against production model
  - Alert if any metric drops >5% from baseline
  - Automated rollback if critical safety metrics (crisis recall) drop >2%
- **Shadow Deployment Strategy:**
  - New model versions run in parallel (shadow mode) for 2 weeks
  - Compare outputs against current production model
  - Require manual approval if outputs diverge >10%
- **Evolving Test Sets:**
  - Quarterly review of new slang/language patterns (TikTok, Discord trends)
  - Add adversarial examples from false negative post-mortems
  - Maintain minimum 10,000 test cases across all risk levels
- **A/B Testing Framework:**
  - When deploying model updates, use gradual rollout (5% â†’ 25% â†’ 100%)
  - Compare safety metrics between cohorts before full deployment

**Success Metric:** Zero undetected degradation >5% for >7 days

---

### 1.3 Adversarial Input Evolution
**What Goes Wrong:** Students discover the system flags "suicide" so they use coded language ("unalive", "toaster bath", new metaphors). Crisis detection becomes ineffective.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Red Team Testing:**
  - Quarterly adversarial testing sessions with actual teenagers (with IRB approval)
  - Hire consultants familiar with current teen online culture
  - Test deliberately evasive language patterns
- **Pattern Update Workflow:**
  - Crisis pattern library versioned in Git
  - Monthly review cycle for new patterns
  - Automated tests ensure new patterns don't increase false positives >1%
- **Community Reporting:**
  - Counselors can flag "this should have been caught" cases
  - Automated pattern extraction from flagged cases
  - Monthly review meeting to update crisis keyword database

**Success Metric:** Crisis recall >99% on adversarial test set, false positive rate <2%

---

## 2. OPERATIONAL FAILURES

### 2.1 Crisis Alert Routing Failures
**What Goes Wrong:** Student in crisis at 3am, system flags it, notification goes to counselor's work email checked at 9am. Or SMS sent but phone was off. Student doesn't get help.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“‹ Process Control
- **Multi-Channel Escalation:**
  - Layer 1 (0-5 min): SMS + Push notification to on-call counselor
  - Layer 2 (5-10 min): Phone call to on-call counselor + backup counselor
  - Layer 3 (10-15 min): Escalate to crisis coordinator + school admin
  - Layer 4 (15+ min): Automated fallback to National Suicide Prevention Lifeline info displayed to student
- **On-Call Rotation System:**
  - Schools must designate 24/7 on-call coverage (or disable after-hours access)
  - System enforces confirmation: "Are you available for crisis alerts today? Y/N"
  - If counselor marks unavailable, system requires backup designation
- **Acknowledgment Enforcement:**
  - Crisis alerts require explicit ACK within 5 minutes
  - If no ACK, escalate to next layer automatically
  - Track time-to-acknowledgment as compliance metric
- **Fallback to Manual Process:**
  - If notification service is down, system displays direct crisis resources to student immediately
  - Email backup sent to school's crisis distribution list
  - Weekly drills to test notification paths

**Success Metric:** Crisis acknowledgment <5 minutes, 99.9% delivery rate

---

### 2.2 Service Downtime Impact
**What Goes Wrong:** AWS region outage, system goes down. Students in distress have nowhere to turn because they relied on the AI being available.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“‹ Process Control
- **Graceful Degradation:**
  - Static page with crisis resources (988, Crisis Text Line) loads even if backend is down
  - Browser-cached emergency contact info
  - "System unavailable - please contact [school counselor name/number]" with school-specific numbers
- **Multi-Region Deployment:**
  - Active-passive setup: Primary in us-east-1, standby in us-west-2
  - Health checks every 30 seconds, automatic failover <2 minutes
- **Offline Mode for Counselor Dashboard:**
  - Critical student lists cached locally
  - View-only mode for recent assessments
  - Cannot create new entries, but can view existing data
- **Status Communication:**
  - Public status page (status.feelwell.com)
  - In-app banner when operating in degraded mode
  - Email to school admins when outage detected

**Success Metric:** 99.9% uptime, <2 min failover time, zero student-facing errors without fallback

---

### 2.3 Alert Fatigue
**What Goes Wrong:** System flags too many students as "caution" level. Counselors get 50 alerts/day, become numb, miss the actual crisis buried in the noise.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Intelligent Alert Aggregation:**
  - Don't send alert for every message - aggregate session-level assessments
  - "Student X's risk increased from SAFE to CAUTION over 3 sessions" (not 10 individual alerts)
  - Suppress repeat alerts for same student within 24hr unless escalation occurs
- **Priority Ranking:**
  - CRISIS: Immediate alert (always)
  - CAUTION + Escalating trajectory: Daily digest
  - CAUTION + Stable: Weekly digest
  - SAFE: No alert
- **Threshold Tuning:**
  - Track alert-to-action ratio (how often counselors act on an alert)
  - If ratio <30%, raise CAUTION threshold
  - School-specific tuning based on counselor capacity
- **Workload-Aware Alerting:**
  - Track counselor caseload (via system usage)
  - If counselor has >20 active CAUTION cases, raise bar for new flags
  - Distribute alerts across counselor team if one is overwhelmed

**Success Metric:** Alert-to-action ratio >40%, counselor satisfaction with alert volume >4/5

---

## 3. DATA & PRIVACY FAILURES

### 3.1 Data Breach / Unauthorized Access
**What Goes Wrong:** Attacker gains access to conversation logs, PHQ-9 scores, or PII. Student mental health data exposed publicly.

**Mitigation Strategy:** ðŸ”’ Technical Control
- **Encryption:**
  - At rest: AES-256 for all data stores (RDS, DocumentDB, S3)
  - In transit: TLS 1.3 minimum for all API calls
  - Field-level encryption for PII (student names, emails)
- **Access Controls:**
  - Zero trust architecture: All service-to-service calls require mutual TLS
  - RBAC: Counselors can only see students at their school
  - Database credentials rotated every 90 days via AWS Secrets Manager
  - No production database access for engineers (read-only replicas only)
- **Audit Logging:**
  - Every data access logged (VIEW_CONVERSATION, VIEW_SCORES, EXPORT_DATA)
  - Immutable audit trail (QLDB or WORM-enabled PostgreSQL)
  - Automated anomaly detection: Alert if counselor views >50 records/day
- **Breach Response Plan:**
  - Incident response playbook with <24hr notification requirement
  - Pre-drafted breach notification templates (FERPA-compliant)
  - Cyber insurance coverage

**Success Metric:** Zero unauthorized access incidents, 100% encryption coverage, <1hr detection time for anomalies

---

### 3.2 Audit Log Gaps / Compliance Drift
**What Goes Wrong:** System changes log format, or deployment breaks event stream. 18 months later during an audit, logs are incomplete or unreadable. Can't prove compliance.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Schema Versioning:**
  - Audit log schema versioned (v1, v2, etc.)
  - Old log readers maintained for 7 years
  - Migration scripts tested before schema changes
- **Completeness Monitoring:**
  - Heartbeat events every hour (proves logging is working)
  - Count critical events: If #(CRISIS_DETECTED) in logs â‰  #(crisis flags in DB), alert
  - Daily audit log completeness check (compare expected vs actual event counts)
- **Change Control for Compliance-Critical Systems:**
  - Audit service changes require security review + compliance sign-off
  - Cannot deploy audit service changes without passing compliance test suite
  - Artifact tracking: Maintain document of "when we changed what" with approval signatures
- **Automated Compliance Testing:**
  - Weekly: Run synthetic compliance scenario (simulate crisis, verify all required logs exist)
  - Monthly: External compliance check (can we generate all required reports?)
  - Quarterly: Legal review of audit log retention policies

**Success Metric:** 100% audit log completeness, zero compliance violations in external audits

---

### 3.3 Data Retention Policy Violations
**What Goes Wrong:** FERPA requires 7-year retention, but also requires deletion upon request. System deletes data too early, or keeps it too long. Compliance violations.

**Mitigation Strategy:** ðŸ”’ Technical Control
- **Automated Lifecycle Policies:**
  - S3 lifecycle rules: Standard (0-30d) â†’ Glacier (30d-7yr) â†’ Delete (7yr)
  - Database records auto-archived to cold storage at 90 days
  - Deletion requests override retention: Immediate deletion + tombstone record for audit
- **Legal Hold System:**
  - If data is subject to litigation, mark with legal hold flag
  - Automated deletion skips legal-hold records
  - Quarterly legal review of holds (can we release yet?)
- **Right to Be Forgotten:**
  - Student/parent can request deletion via authenticated portal
  - 30-day verification period (email confirmation)
  - Hard delete from all systems + confirmation email
  - Audit log retains "deletion occurred" record (no PII)
- **Retention Policy Monitoring:**
  - Monthly report: Count records exceeding 7 years
  - Automated alerts if data >7yr+30days exists (grace period for deletions)

**Success Metric:** Zero retention policy violations, 100% deletion request fulfillment within 30 days

---

### 3.4 Geographic Expansion Without Compliance Review
**What Goes Wrong:** European school wants to use the system. GDPR requires data stays in EU, but architecture is hard-coded to AWS us-east-1. Can't support them, or worse, violate GDPR unknowingly.

**Mitigation Strategy:** ðŸ“‹ Process Control + ðŸ”’ Technical Control
- **Decision Log:**
  - Create DECISION_LOG.md documenting: "V1 supports USA only due to FERPA/COPPA focus"
  - Trigger for review: "Before expanding to new country, revisit this decision"
  - Track jurisdictions we explicitly do NOT support yet
- **Architecture Decision Trigger:**
  - ADR template includes "Geographic Scope" section
  - When writing ADRs, explicitly state: "This decision assumes US-only deployment"
  - Tagging: Tag ADRs with #geo-specific if they have locality assumptions
- **Pre-Expansion Checklist:**
  - Before launching in new country:
    1. Legal review of data protection laws (GDPR, local equivalents)
    2. Infrastructure review: Do we need in-country hosting?
    3. Audit all geo-specific ADRs - which need updates?
    4. Compliance certification: What certifications needed? (ISO 27001, etc.)
- **Technical Guardrails:**
  - School signup form requires country selection
  - If country â‰  USA, block signup with: "Not available in your region yet - join waitlist"
  - Prevents accidental international deployment

**Success Metric:** Zero unauthorized international deployments, 100% compliance in all supported regions

---

## 4. BIAS & ACCESSIBILITY FAILURES

### 4.1 Selection Bias in Usage Data
**What Goes Wrong:** Only tech-comfortable students use the system. Analytics show "20% of students have anxiety" but it's really "20% of tech-savvy students." Schools make resource decisions based on incomplete data, underserving non-users.

**Mitigation Strategy:** ðŸ“Š Monitoring + ðŸ“‹ Process Control
- **Usage Bias Monitoring:**
  - Track demographics of users vs school population (if school provides aggregate demographics)
  - Alert if: % using system differs >15% from school demographics (by grade, gender, ESL status)
  - Dashboard prominently displays: "Data represents X% of student body"
- **Multi-Channel Intake Strategy:**
  - Don't rely solely on AI chat - offer:
    1. AI chat (for tech-comfortable students)
    2. Anonymous paper survey (for tech-averse students)
    3. QR codes in bathrooms/common areas (low-barrier entry)
    4. Counselor-initiated check-ins (catch non-users)
  - Aggregate risk data across all channels, not just AI
- **Proactive Outreach Campaigns:**
  - If student hasn't used system in 90 days, counselor can initiate offline check-in
  - "Not using the app â‰  doing fine" - train counselors to not assume
- **Sampling Audits:**
  - Quarterly: Random sample of non-users for in-person well-being check
  - Compare risk levels of users vs non-users (within ethical limits)
  - If non-users have higher crisis rates, indicates selection bias
- **Reporting Transparency:**
  - All reports include disclaimer: "Based on students who engaged with system (X% of population)"
  - Highlight underrepresented groups in usage stats
  - Recommendations for reaching non-users

**Success Metric:** Usage within Â±10% of school demographics, schools implement multi-channel strategies

---

### 4.2 Language and Cultural Bias
**What Goes Wrong:** System works great for native English speakers but fails for ESL students, students with learning disabilities, or those from cultures that express distress differently. Misses real crises.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Multi-Language Support:**
  - Phase 1: English + Spanish (high priority for US schools)
  - Phase 2: Add top 5 languages in target districts
  - Crisis keywords translated by native speakers + mental health professionals (not just Google Translate)
- **Accessibility Features:**
  - Text-to-speech for students with reading difficulties
  - Speech-to-text for typing difficulties
  - Adjustable reading level (simple language mode)
  - Image-based emotion selection for non-verbal expression
- **Cultural Sensitivity Review:**
  - Partner with culturally diverse mental health experts
  - Review model outputs for cultural bias (e.g., stoicism in some cultures â‰  lack of distress)
  - Maintain cultural context database: "In X culture, Y phrase means Z"
- **Bias Testing:**
  - Create test sets for each supported demographic group
  - Measure accuracy across: ESL status, learning disability status, cultural background
  - Require: Max 10% accuracy difference between groups
- **Performance Monitoring by Subgroup:**
  - Track crisis recall rate by: Language, ESL status, disability accommodations
  - Alert if any subgroup has <95% recall (vs >99% overall)

**Success Metric:** <10% accuracy variance across demographic groups, 95%+ recall for all subgroups

---

### 4.3 Accessibility for Non-Text Communicators
**What Goes Wrong:** Student communicates primarily through memes, emojis, or short phrases. Text-based NLP fails to understand. System marks them as low-risk when they're actually struggling.

**Mitigation Strategy:** ðŸ”’ Technical Control (Future) + ðŸ“‹ Process Control (Now)
- **Multi-Modal Input (Future):**
  - Accept image uploads (analyze memes for dark humor / concerning content)
  - Emoji sentiment analysis
  - Voice input with emotion detection
- **Current Workarounds:**
  - Provide structured check-ins: "Rate your mood 1-10" with emoji scale
  - Multiple choice symptom checklist (doesn't require articulation)
  - "Select images that match your feelings" (visual assessment)
- **Human Escalation for Low-Engagement:**
  - If student sends <5 words per session, flag for counselor review
  - "This student may need alternative assessment method"
  - Don't penalize silence with "low risk" label

**Success Metric:** (TBD - need more research on non-text users)

---

## 5. ETHICAL & LEGAL FAILURES

### 5.1 Data Becomes Permanent Record
**What Goes Wrong:** Student has rough week, uses system, improves, graduates. Five years later during background check or insurance application, that data resurfaces or is requested. Stigmatizes them permanently.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“‹ Process Control
- **Data Minimization:**
  - Don't store raw conversation text beyond 90 days (only clinical scores + summaries)
  - After 90 days: Delete messages, keep only aggregate risk scores
  - After 7 years (FERPA minimum): Hard delete everything
- **No Third-Party Sharing:**
  - Terms of Service explicitly prohibit: Selling data, sharing with insurers, providing to background check companies
  - Legal review required before any new data sharing partnership
- **Right to Deletion:**
  - Student or parent can request full deletion anytime
  - Upon graduation, proactive email: "Would you like us to delete your data?"
  - Default: Delete 1 year after graduation unless student opts to keep
- **No Data Portability for Mental Health:**
  - Do NOT allow students to export their full mental health history
  - Prevents accidental self-harm (sharing with wrong parties)
  - Exception: Student can request clinical summary sent to their personal therapist
- **Contractual Protections:**
  - Schools sign DPA (Data Processing Agreement) prohibiting sharing student data
  - Insurance companies, background check firms added to blocklist
  - Legal action clause if school violates

**Success Metric:** Zero third-party data sharing incidents, 100% deletion request fulfillment

---

### 5.2 Liability Ambiguity
**What Goes Wrong:** Student crisis occurs despite using system. School blames vendor ("AI should've caught it"), vendor blames counselor ("we flagged it, they ignored"), counselor blames school ("I wasn't trained"). Nobody accountable.

**Mitigation Strategy:** ðŸ“‹ Process Control
- **Clear Responsibility Matrix (RACI):**
  - **System Responsibility:** Detect potential risk, deliver notifications
  - **Counselor Responsibility:** Acknowledge alerts, conduct assessment, take action
  - **School Responsibility:** Ensure counselor training, maintain on-call coverage, verify system is used as supplement not replacement
- **Terms of Service Clarity:**
  - Explicit disclaimer: "This is a triage tool, not a diagnostic tool"
  - "System may miss crises - counselors must use professional judgment"
  - "Schools must maintain adequate counseling staff - this is a supplement, not replacement"
- **Training Requirements:**
  - Schools must certify: All counselors trained on system (2-hour training)
  - Training includes: How to interpret flags, when to override, escalation protocols
  - Annual recertification required
  - Cannot activate system until training certified
- **Audit Trail of Actions:**
  - System logs: Alerts sent, alerts acknowledged, alerts overridden
  - If crisis occurs, can reconstruct: Did system flag? Did counselor see? What action was taken?
  - Not for blame - for process improvement
- **Post-Incident Review Protocol:**
  - After serious incident: Convene review (school + vendor)
  - Focus on: What did we learn? How do we prevent next time?
  - Update training or system based on findings

**Success Metric:** Zero liability disputes, 100% school training compliance

---

### 5.3 Mission Drift: Replacement Not Supplement
**What Goes Wrong:** Administrators see this as cost savings - "We can cut counselor headcount, we have AI now." System becomes replacement instead of force multiplier. Quality of care decreases.

**Mitigation Strategy:** ðŸ“‹ Process Control + ðŸ“Š Monitoring
- **Contract Guardrails:**
  - Require schools to certify: Counselor-to-student ratio meets minimum standards (e.g., 1:250 per ASCA)
  - If ratio worsens during contract term, requires vendor notification + justification
  - Vendor reserves right to terminate if used as counselor replacement
- **Sales Process Integrity:**
  - Sales team trained: Never position as counselor replacement
  - Marketing materials explicitly state: "Empowers counselors, does not replace them"
  - If sales rep violates, disciplinary action
- **Usage Monitoring:**
  - Track: Counselor-to-student ratio at signup vs 1 year later
  - If ratio drops >20%, trigger conversation with school
  - Dashboard shows: "This school's counselor ratio changed from 1:300 to 1:450"
- **Student Outcome Tracking:**
  - Survey students: "Did you get the help you needed?"
  - If satisfaction drops, investigate: Is it staffing cuts?
- **Industry Advocacy:**
  - Publicly advocate for counselor hiring, not cuts
  - Publish white papers: "AI as supplement, not substitute"

**Success Metric:** Zero schools using system to justify counselor cuts (requires monitoring + enforcement)

---

## 6. HUMAN FACTORS (Monitoring-Only)

### 6.1 Learned Helplessness
**What Goes Wrong:** Students treat AI as "the solution" instead of a triage tool. They think "I talked to the AI, I'm good" rather than seeking real help. AI becomes barrier to care.

**Mitigation Strategy:** ðŸ“Š Monitoring + ðŸ“‹ Process Control
- **System Messaging:**
  - Every 5th message: "Remember, I'm here to help you connect with support - have you considered talking to [counselor name]?"
  - After CAUTION flag: "I've noticed you're struggling. I'd like to have your counselor reach out. Is that okay?"
  - Never position as "therapy" - always "triage" and "bridge to support"
- **Counselor Engagement Metrics:**
  - Track: % of flagged students who actually meet with counselor
  - If <50% follow through, indicates students think AI is enough
- **Student Surveys:**
  - Quarterly: "Do you think talking to the AI is the same as talking to a counselor?"
  - If >20% say "yes", adjust messaging
- **Proactive Counselor Outreach:**
  - Counselors contact flagged students within 48 hours (system doesn't wait for student to take action)
  - "I saw you've been using Feelwell - how about we chat in person?"
- **Outcome Tracking:**
  - Long-term: Do students who use AI-only have worse outcomes than AI + counselor?
  - If yes, need to change engagement model

**Success Metric:** >70% of flagged students meet with counselor, <10% view AI as replacement for human support

---

### 6.2 Counselor Resistance / Fear of Replacement
**What Goes Wrong:** Counselors see AI as threat to jobs, subtly sabotage it (don't use it, bad-mouth it to students, ignore alerts). System fails due to human resistance.

**Mitigation Strategy:** ðŸ“‹ Process Control + ðŸ“Š Monitoring
- **Positioning:**
  - Frame as: "This gives you back time for high-touch support"
  - Highlight: "You spend less time on intake screening, more time on actual counseling"
  - Never: "This makes you more efficient" (sounds like layoff justification)
- **Counselor Input:**
  - Include counselors in design process: "What would make your job easier?"
  - Feature requests from counselors prioritized
  - Annual counselor advisory board (paid, respected, listened to)
- **Job Security Messaging:**
  - Exec team public commitment: "We are not replacing counselors, we are supporting them"
  - Contract terms prohibit using system to justify staff reductions
- **Training & Onboarding:**
  - Not just "how to use" but "why this helps you"
  - Share success stories from other counselors
  - Peer-to-peer training (counselors train counselors)
- **Monitoring Resistance:**
  - Track: Counselor login frequency, alert acknowledgment rate
  - If counselor hasn't logged in for 14 days, check-in: "Are you having trouble with the system?"
  - Exit interviews: If counselor leaves, ask about AI tool (was it factor?)

**Success Metric:** >80% counselor adoption, counselor satisfaction >4/5, zero counselor departures citing AI as reason

---

### 6.3 Student Trust Erosion
**What Goes Wrong:** Student shares something sensitive, later sees counselor who mentions it. Student feels betrayed ("I thought this was private"), stops using system or worse, stops seeking any help.

**Mitigation Strategy:** ðŸ“‹ Process Control + ðŸ”’ Technical Control
- **Transparency About Data Sharing:**
  - First-time user: "I'll share concerning patterns with your counselor to help you. Is that okay?"
  - Before crisis alert: "I'm worried about you and need to involve your counselor. Here's what I'll share: [preview]"
  - After every session: "Your counselor can see: Risk level, topics discussed. They cannot see: Exact messages (unless you give permission)."
- **Student Control:**
  - "Private mode": Student can mark certain sessions as "don't flag counselor unless crisis"
  - Clear UI: Green = shared with counselor, Yellow = private, Red = crisis (must share)
- **Counselor Training:**
  - How to approach flagged students without revealing exact words
  - "I noticed you've been feeling down lately" NOT "You told the AI you've been feeling down"
  - Build trust, don't break it
- **Trust Metrics:**
  - Survey: "Do you trust Feelwell with your feelings?" (quarterly)
  - Track: % of students who stop using after first counselor interaction (should be <5%)
  - If trust score drops, investigate: What changed?

**Success Metric:** Student trust score >4/5, <5% abandonment after counselor contact

---

## 7. TECHNICAL DEBT & SCALABILITY

### 7.1 Infrastructure Incompleteness
**What Goes Wrong:** Only 15-20% of architecture implemented. Rush to launch, skip critical pieces like encryption, auth, or event infrastructure. Security vulnerabilities or data loss.

**Mitigation Strategy:** ðŸ“‹ Process Control
- **Go/No-Go Checklist for V2 Launch:**
  - âŒ Cannot launch without: Auth service, encryption at rest, audit logging, crisis notification system
  - âš ï¸ Should have but can defer: Analytics service, k-anonymity, multi-region
  - âœ… Nice to have: LLM fine-tuning, advanced dashboards
- **Technical Debt Tracking:**
  - Maintain TECH_DEBT.md with severity levels (Critical, High, Medium, Low)
  - Critical: Must fix before launch
  - High: Must fix within 90 days of launch
  - Monthly review: Are we paying down debt or accumulating?
- **Architecture Review Gate:**
  - Before launch, external security audit (penetration test)
  - Compliance audit (can we pass SOC 2 Type 1?)
  - Load testing: Can we handle 10,000 concurrent users?

**Success Metric:** Zero critical tech debt at launch, <10 high-priority items

---

### 7.2 Scalability Bottlenecks
**What Goes Wrong:** System works fine with 1,000 students at pilot school. Launch to district with 50,000 students, database collapses under load. Outage during crisis.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Load Testing:**
  - Before launch: Test at 10x expected peak load
  - Automated load tests in CI/CD pipeline (catch regressions)
- **Auto-Scaling:**
  - All services in ECS/EKS with auto-scaling enabled
  - Database: Read replicas for queries, write scaling via partitioning
  - Crisis detection: Dedicated high-priority queue (never blocked by regular traffic)
- **Circuit Breakers:**
  - If LLM service is slow (>5s response), return safe fallback response
  - If database is overloaded, return cached data + degraded mode notice
- **Capacity Planning:**
  - Track growth: Students onboarded per month
  - When at 70% capacity, trigger infrastructure scaling
  - Quarterly capacity review: Are we ahead of growth curve?

**Success Metric:** <2s p95 latency, zero outages due to load, auto-scaling prevents manual intervention

---

## 8. INTEGRATION & INTEROPERABILITY

### 8.1 SIS (Student Information System) Integration Failures
**What Goes Wrong:** Integration with school's SIS breaks (API change, auth expires). Student roster becomes stale. New students can't access system, or graduated students still in database.

**Mitigation Strategy:** ðŸ”’ Technical Control + ðŸ“Š Monitoring
- **Graceful Degradation:**
  - If SIS sync fails, system continues working with stale data
  - Banner: "Student roster last updated: 7 days ago"
  - Manual roster upload as backup
- **Integration Health Monitoring:**
  - Daily sync health check: Did we successfully pull roster?
  - Alert if sync fails 2 consecutive days
  - Test integration monthly (automated test student)
- **Version Pinning:**
  - Use stable API versions (don't auto-upgrade)
  - 90-day notice before deprecating old integration version
  - Maintain backward compatibility for 2 versions
- **Multi-SIS Support:**
  - Don't hard-code for one SIS (e.g., PowerSchool)
  - Adapter pattern: Support Clever, ClassLink, Skyward, etc.
  - If school doesn't have SIS, allow manual roster management

**Success Metric:** 99%+ SIS sync success rate, <1 day staleness

---

## Summary: Mitigation Strategy Breakdown

| Failure Mode Category | Technical Controls | Monitoring Mechanisms | Process Controls |
|------------------------|-------------------|----------------------|------------------|
| **Trust & Transparency** | 3 | 3 | 1 |
| **Operational** | 4 | 1 | 3 |
| **Data & Privacy** | 5 | 2 | 2 |
| **Bias & Accessibility** | 3 | 3 | 2 |
| **Ethical & Legal** | 2 | 1 | 5 |
| **Human Factors** | 1 | 3 | 3 |
| **Technical Debt** | 0 | 1 | 2 |
| **Integration** | 2 | 1 | 0 |
| **TOTAL** | **20** | **15** | **18** |

---

## Next Steps for V2 Design

1. **Prioritize Controls:** Which technical controls are MVP requirements vs future enhancements?
2. **Monitoring Infrastructure:** Build observability stack (what metrics matter most?)
3. **Process Documentation:** Create runbooks, training materials, incident response plans
4. **Compliance Mapping:** Map controls to FERPA, COPPA, SOC 2 requirements
5. **Threat Model:** Formal STRIDE analysis based on these failure modes
6. **User Stories:** Translate mitigations into user-facing features

---

**Document Status:** Draft for review
**Next Review:** After V2 design discussion
